{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "690380bb",
      "metadata": {
        "id": "690380bb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9caa332",
      "metadata": {
        "id": "a9caa332"
      },
      "source": [
        "# Single Neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46511047",
      "metadata": {
        "id": "46511047"
      },
      "source": [
        "## Example- The Linear Unit as a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74eed424",
      "metadata": {
        "id": "74eed424"
      },
      "source": [
        "Below is a multiple inputs single neuron:    \n",
        "<img src=\"https://i.imgur.com/vyXSnlZ.png\" width=\"50%\">\n",
        "\n",
        "The formula for this neuron would be: $y=w_0x_0+w_1x_1+w_2x_2+b$\n",
        "* $x_0, x_1, x_1$: inputs\n",
        "* $w_0, w_1, w_1$：weights. Whenever a value flows through a connection, you multiply the value by the connection's **weight**. For the input **x**, what reaches the neuron is **w * x**. A neural network \"learns\" by modifying its **weights**.\n",
        "* $b$: a special kind of weight we call the **bias**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc353d4d",
      "metadata": {
        "id": "dc353d4d"
      },
      "source": [
        "## Linear Units in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c5f127",
      "metadata": {
        "id": "73c5f127"
      },
      "source": [
        "We could define a linear model accepting three input features($x_0, x_1, x_1$)and producing a single output($y$) like below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a9e5dcd0",
      "metadata": {
        "id": "a9e5dcd0"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0b268fd8",
      "metadata": {
        "id": "0b268fd8"
      },
      "outputs": [],
      "source": [
        "# Create a network with 1 linear unit\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(units=1, input_shape=[3])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e12a73",
      "metadata": {
        "id": "91e12a73"
      },
      "source": [
        "* with the first arguement **units**, we difine how many outputs we want. **It is equal to the number of neurons in a layer**. Here the output is only $y$.\n",
        "* with the second arguement **input_shape**, we tell Keras the dimensions of the inputs. Here the inputs are $x_0, x_1, x_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbdcca44",
      "metadata": {
        "id": "fbdcca44"
      },
      "source": [
        "# Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bee1e7e",
      "metadata": {
        "id": "6bee1e7e"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480cfbdb",
      "metadata": {
        "id": "480cfbdb"
      },
      "source": [
        "Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.  \n",
        "\n",
        "<img src=\"https://i.imgur.com/2MA4iMV.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5529b3",
      "metadata": {
        "id": "1b5529b3"
      },
      "source": [
        "## Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0809f435",
      "metadata": {
        "id": "0809f435"
      },
      "source": [
        "An activation function is simply some function we apply to each of a layer's outputs (its activations). The most common is the rectifier function  max(0,x).  \n",
        "\n",
        "<img src=\"https://i.imgur.com/aeIyAlF.png\">\n",
        "\n",
        "**Without activation functions, neural networks can only learn linear relationships**. In order to fit curves, we'll need to use activation functions. The rectifier function has a graph that's a line with the negative part \"rectified\" to zero. Applying the function to the outputs of a neuron will put a bend in the data, moving us away from simple lines.  \n",
        "\n",
        "When we attach the rectifier to a linear unit, we get a **rectified linear unit or ReLU**. (For this reason, it's common to call the rectifier function the \"ReLU function\".) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f528d7",
      "metadata": {
        "id": "f7f528d7"
      },
      "source": [
        "## Stacking Dense Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72499d0",
      "metadata": {
        "id": "e72499d0"
      },
      "source": [
        "<a id = \"chapter_2.3\"></a> \n",
        "Now that we have some nonlinearity, let's see how we can stack layers to get complex data transformations.  \n",
        "\n",
        "<img src=\"https://i.imgur.com/Y5iwFQZ.png\">\n",
        "\n",
        "The layers before the output layer are sometimes called **hidden**.  \n",
        "\n",
        "Notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8899be",
      "metadata": {
        "id": "0d8899be"
      },
      "source": [
        "## Building Sequential Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b27877e",
      "metadata": {
        "id": "7b27877e"
      },
      "source": [
        "The model below is built according to the picture in [Chapter 2.3](#chapter_2.3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "92effba8",
      "metadata": {
        "id": "92effba8"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "edb74585",
      "metadata": {
        "id": "edb74585"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    # the hidden ReLU layers\n",
        "    # the first hidden layer\n",
        "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
        "    #the second hidden layer\n",
        "    layers.Dense(units=3, activation='relu'),\n",
        "    # the linear output layer \n",
        "    layers.Dense(units=1),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f376432",
      "metadata": {
        "id": "4f376432"
      },
      "source": [
        "**Be sure to pass all the layers together in a list, like [layer, layer, layer, ...].**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed86c95b",
      "metadata": {
        "id": "ed86c95b"
      },
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03731dd1",
      "metadata": {
        "id": "03731dd1"
      },
      "source": [
        "## Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2786ca60",
      "metadata": {
        "id": "2786ca60"
      },
      "source": [
        "- Loss Function: it measures how good the network's predictions are, (e.g. MSE, MAE);  \n",
        "\n",
        "\n",
        "- Optimizer: it can tell the network how to change its weights, (e.g stochastic gradient descent);\n",
        "    - SGD Steps:\n",
        "      1. Sample some training data (minibatch) and run it through the network to make predictions;\n",
        "      2. Measure the loss between the predictions and the true values;\n",
        "      3. Finally, adjust the weights in a direction that makes the loss smaller.<br> \n",
        "      <br>\n",
        "    - Minibatch (batch): each iteration's sample of training data;\n",
        "    - Epoch: a complete round of the training data. The number of epochs you train for is how many times the network will see each training example.  \n",
        "    \n",
        "- Learning Rate: A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n",
        "      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1bbf39c",
      "metadata": {
        "id": "c1bbf39c"
      },
      "source": [
        "<img src='https://i.imgur.com/rFI1tIk.gif' width='80%'>\n",
        "The animation shows the linear model  being trained with SGD. \n",
        "The pale red dots depict the entire training set, while the solid red dots are the minibatches.  \n",
        "\n",
        "Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6b94e5",
      "metadata": {
        "id": "fb6b94e5"
      },
      "source": [
        "## Example - Adding the Loss and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfd0dad4",
      "metadata": {
        "id": "bfd0dad4"
      },
      "source": [
        ">model.compile(\n",
        ">    optimizer=\"adam\",\n",
        "    loss=\"mae\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e507d057",
      "metadata": {
        "id": "e507d057"
      },
      "source": [
        "Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c57f58",
      "metadata": {
        "id": "d3c57f58"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6c7f63e5",
      "metadata": {
        "id": "6c7f63e5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "269c78db",
      "metadata": {
        "scrolled": false,
        "id": "269c78db",
        "outputId": "c680e981-a28f-47a1-bc83-0ba280d03f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2eed23ae97ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'red-wine.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'red-wine.csv'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('red-wine.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a2820e",
      "metadata": {
        "id": "e7a2820e"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa33ce7",
      "metadata": {
        "id": "5aa33ce7"
      },
      "source": [
        "### Data Processing\n",
        "<a id = \"chapter_3.2.2\"></a> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2643a36",
      "metadata": {
        "id": "f2643a36"
      },
      "outputs": [],
      "source": [
        "x = data.drop(columns='quality')\n",
        "y = data['quality']\n",
        "x_col_name = x.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2688047d",
      "metadata": {
        "id": "2688047d"
      },
      "source": [
        "##### Spliting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9914a1f6",
      "metadata": {
        "id": "9914a1f6"
      },
      "outputs": [],
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df6b622",
      "metadata": {
        "id": "6df6b622"
      },
      "source": [
        "##### Scaling data\n",
        "neural networks tend to perform best when their inputs are on a common scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c3636d",
      "metadata": {
        "id": "64c3636d"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_train, columns=x_col_name)\n",
        "\n",
        "x_valid = scaler.transform(x_valid)\n",
        "x_valid = pd.DataFrame(x_valid, columns=x_col_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60239b7",
      "metadata": {
        "id": "f60239b7"
      },
      "source": [
        "### Building Model\n",
        "<a id = \"chapter_3.2.3\"></a> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666b7282",
      "metadata": {
        "id": "666b7282"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    # the hidden ReLU layers\n",
        "    # the first hidden layer\n",
        "    layers.Dense(units=512, activation='relu', input_shape=[11]),\n",
        "    #the second hidden layer\n",
        "    layers.Dense(units=512, activation='relu'),\n",
        "    #the third hidden layer\n",
        "    layers.Dense(units=512, activation='relu'),\n",
        "    # the linear output layer \n",
        "    layers.Dense(units=1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a668cdb2",
      "metadata": {
        "scrolled": true,
        "id": "a668cdb2"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='mae')\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e448f7f5",
      "metadata": {
        "id": "e448f7f5"
      },
      "outputs": [],
      "source": [
        "model_history = pd.DataFrame(history.history)\n",
        "model_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3737f9",
      "metadata": {
        "scrolled": true,
        "id": "cc3737f9"
      },
      "outputs": [],
      "source": [
        "model_history.plot(\n",
        "    figsize=(8,5), \n",
        "    title='Learning Curve',\n",
        "    xlabel='Epoch',\n",
        "    ylabel='MAE',\n",
        "    fontsize=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5bc587",
      "metadata": {
        "id": "2c5bc587"
      },
      "source": [
        "# Overfitting and Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40da9d3f",
      "metadata": {
        "id": "40da9d3f"
      },
      "source": [
        "## Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744023b0",
      "metadata": {
        "id": "744023b0"
      },
      "source": [
        "- Signal: it can help our model make predictions from new data.  \n",
        "\n",
        "\n",
        "- Noise: it is only true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. The noise is the part might look useful but really isn't.  \n",
        "\n",
        "\n",
        "- Learning Curves: plot traing loss and validation loss against epochs.the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal.\n",
        "<img src=\"https://i.imgur.com/tHiVFnM.png\" width=\"50%\">\n",
        "\n",
        "- Underfitting： the model dosen't learn enough signal.   \n",
        "\n",
        "\n",
        "- Overfitting the model learns too much noise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46757934",
      "metadata": {
        "id": "46757934"
      },
      "source": [
        "## Ways to Decrease Overfitting and Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb1e759",
      "metadata": {
        "id": "7fb1e759"
      },
      "source": [
        "### Capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00d13595",
      "metadata": {
        "id": "00d13595"
      },
      "source": [
        "A model's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, it is determined by the how many neurons it has and how they are connected together. If your model is underfitting, you should increase the capacity.  \n",
        "\n",
        "\n",
        "Methods to increase capacity:\n",
        "- Winder networks: have easier time learning more linear relationship.\n",
        "- Deeper networks: prefer more nonlinear relationship.\n",
        "\n",
        "    Examples of wider and deeper networks:\n",
        "```Python\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "wider = keras.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "deeper = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70e633c",
      "metadata": {
        "id": "f70e633c"
      },
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e3d776",
      "metadata": {
        "id": "36e3d776"
      },
      "source": [
        "#### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48677ab2",
      "metadata": {
        "id": "48677ab2"
      },
      "source": [
        "Simply stop the training whenever it seems the validation loss isn't decreasing anymore. \n",
        "<img src=\"https://i.imgur.com/eP0gppr.png\" width=\"50%\">\n",
        "\n",
        "Code:\n",
        "```Python\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.01, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "```\n",
        "These parameters say: \"If there hasn't been at least an improvement of 0.01 in the validation loss over the previous 20 epochs, then stop the training and keep the best model you found.\" It can sometimes be hard to tell if the validation loss is rising due to overfitting or just due to random batch variation. The parameters allow us to set some allowances around when to stop.<p>\n",
        "**<font color=red size=6>how to understand EarlyStopping?</font>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "585a2089",
      "metadata": {
        "id": "585a2089"
      },
      "source": [
        "#### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a56850",
      "metadata": {
        "id": "80a56850"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29314b3",
      "metadata": {
        "id": "e29314b3"
      },
      "source": [
        "Here still use the model built in [Chapter 3.2.3](#chapter_3.2.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f259368c",
      "metadata": {
        "id": "f259368c"
      },
      "source": [
        "##### Set EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef1aa405",
      "metadata": {
        "id": "ef1aa405"
      },
      "outputs": [],
      "source": [
        "early_stopping = callbacks.EarlyStopping(\n",
        "    min_delta=0.01, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513cfd2d",
      "metadata": {
        "id": "513cfd2d"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    epochs=100,\n",
        "    batch_size=256,\n",
        "    callbacks=[early_stopping],   # put your callbacks in a list\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abbd574c",
      "metadata": {
        "scrolled": true,
        "id": "abbd574c"
      },
      "outputs": [],
      "source": [
        "model_history = pd.DataFrame(history.history)\n",
        "\n",
        "val_loss_min = model_history.val_loss.min()\n",
        "epoch_min = model_history[model_history.val_loss == val_loss_min].index\n",
        "val_loss_min = round(val_loss_min, 4)\n",
        "epoch_min = np.array(epoch_min)[0]\n",
        "print('Minimun validation loss: {}'.format(val_loss_min))\n",
        "print('Epoch number: {}'.format(epoch_min))\n",
        "\n",
        "_,ax1 = plt.subplots()\n",
        "model_history.plot(\n",
        "    figsize=(10,6), \n",
        "    xlabel='Epoch',\n",
        "    ylabel='MAE',\n",
        "    fontsize=10,\n",
        "    ax=ax1,\n",
        "    xticks=np.arange(0, 26, 5),\n",
        "    yticks=np.arange(0, 0.6, 0.05)\n",
        ")\n",
        "ax1.set_title('Learning Curves', size=30)\n",
        "ax1.scatter(epoch_min, val_loss_min, color='green',s=80)\n",
        "ax1.grid(linestyle='--')\n",
        "ax1.text(epoch_min, val_loss_min, '({},{})'.format(epoch_min, val_loss_min),size=15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d4ca65",
      "metadata": {
        "id": "40d4ca65"
      },
      "source": [
        "It shows that the Keras stopped training before the full 100 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b3508b",
      "metadata": {
        "id": "f8b3508b"
      },
      "source": [
        "# Dropout and Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72afa88",
      "metadata": {
        "id": "f72afa88"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae53767d",
      "metadata": {
        "id": "ae53767d"
      },
      "source": [
        "Randomly drop out some fraction of a layer's input units **every step of training**, making it much harder for the network to learn those spurious patterns in the **training data**. <p>\n",
        "You could also think about dropout as creating a kind of ensemble of networks. The predictions will no longer be made by one big network, but instead by **a committee of smaller networks**. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you're familiar with **random forests** as an ensemble of decision trees, it's the same idea.)<p>\n",
        "Below picture shows 50% dropout has been added between the two hidden layers.\n",
        "<img src=\"https://i.imgur.com/a86utxY.gif\" width='80%'>\n",
        "\n",
        "    \n",
        "Code:<p>\n",
        "```Python\n",
        "keras.Sequential([\n",
        "    # ...\n",
        "    # Put the Dropout layer just before the layer you want the dropout applied to\n",
        "    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer \n",
        "    layers.Dense(16),\n",
        "    # ...\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf744e1",
      "metadata": {
        "id": "5bf744e1"
      },
      "source": [
        "## Batch Normalization (Batchnorm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b684bcce",
      "metadata": {
        "id": "b684bcce"
      },
      "source": [
        "Batchnorm can help correct training that is slow or unstable.The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.<p>\n",
        "A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.<p>\n",
        "it's good to normalize the data before it goes into the network:\n",
        "```Python\n",
        "layers.BatchNormalization(),\n",
        "layers.Dense(16, activation='relu'),\n",
        "keras.Sequential([\n",
        "    # first layer\n",
        "    layers.BatchNormalization(), # act as a kind of adaptive preprocessor \n",
        "    layers.Dense(16),\n",
        "    # ...\n",
        "])\n",
        "```\n",
        "\n",
        "Most often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance). Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get \"stuck\". Consider adding batch normalization to your models, especially if you're having trouble during training.<p>\n",
        "It seems that batch normalization can be used at almost any point in a network. You can put it after a layer...\n",
        "```Python\n",
        "layers.Dense(16, activation='relu'),\n",
        "layers.BatchNormalization(),\n",
        " ```\n",
        "... or between a layer and its activation function:\n",
        "```Python\n",
        "layers.Dense(16),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation('relu'),\n",
        "```\n",
        "**<font color=red size=6>What is the influence by adding batchnorm between different layers?</font>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd0df2d",
      "metadata": {
        "id": "0dd0df2d"
      },
      "source": [
        "## Example - Using Dropout and Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64dfbf9",
      "metadata": {
        "id": "c64dfbf9"
      },
      "source": [
        "Here still use the dataset processed in [Chapter 3.2.3](#chapter_3.2.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2a11ad",
      "metadata": {
        "id": "bb2a11ad"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfdc65c",
      "metadata": {
        "id": "0cfdc65c"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a82b38",
      "metadata": {
        "scrolled": true,
        "id": "58a82b38"
      },
      "outputs": [],
      "source": [
        "model_history = pd.DataFrame(history.history)\n",
        "\n",
        "val_loss_min = model_history.val_loss.min()\n",
        "epoch_min = model_history[model_history.val_loss == val_loss_min].index\n",
        "val_loss_min = round(val_loss_min, 4)\n",
        "epoch_min = np.array(epoch_min)[0]\n",
        "print('Minimun validation loss: {}'.format(val_loss_min))\n",
        "print('Epoch number: {}'.format(epoch_min))\n",
        "\n",
        "_,ax2 = plt.subplots()\n",
        "model_history.plot(\n",
        "    figsize=(10,6), \n",
        "    xlabel='Epoch',\n",
        "    ylabel='MAE',\n",
        "    fontsize=10,\n",
        "    ax=ax2,\n",
        "    xticks=np.arange(0, 105, 5),\n",
        ")\n",
        "\n",
        "ax2.set_title('Learning Curves', size=30)\n",
        "ax2.scatter(epoch_min, val_loss_min, color='green',s=80)\n",
        "ax2.grid(linestyle='--')\n",
        "ax2.text(epoch_min, val_loss_min, '({},{})'.format(epoch_min, val_loss_min),size=15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36950773",
      "metadata": {
        "id": "36950773"
      },
      "source": [
        "# Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7538fe47",
      "metadata": {
        "id": "7538fe47"
      },
      "source": [
        "## Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a9bcd83",
      "metadata": {
        "id": "6a9bcd83"
      },
      "source": [
        "- Accuracy: $accuracy = number_{correct} ~/~ total$\n",
        "- Binary Cross-Entropy: $H_p(q) = - \\frac{1}{N}\\Sigma y_i \\cdot log(p(y_i)) + (1-y_i)\\cdot log(1-p(y_i)) $\n",
        "  - $y_i$: 1 for positive, 0 for negative;\n",
        "  -$p(y_i)$: the predicted probability.\n",
        "<img src=\"https://i.imgur.com/DwVV9bR.png\" width=\"50%\">\n",
        "- Sigmoid Activation: $\\sigma (x) = \\frac{1}{1+e^{-x}}$ , it maps real numbers into the interval [0,1].<img src='https://i.imgur.com/FYbRvJo.png' width='48%'>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c90ddd",
      "metadata": {
        "id": "d0c90ddd"
      },
      "source": [
        "## Example - Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1b9980",
      "metadata": {
        "id": "4f1b9980"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1681ab8d",
      "metadata": {
        "id": "1681ab8d"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('ion.csv', index_col=0)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c070ac8d",
      "metadata": {
        "id": "c070ac8d"
      },
      "source": [
        "### Data precessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb6c995",
      "metadata": {
        "id": "feb6c995"
      },
      "source": [
        "##### change the class column into numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8431383",
      "metadata": {
        "id": "a8431383"
      },
      "outputs": [],
      "source": [
        "data.Class.replace(to_replace=['good', 'bad'], value=[1,0], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de3c8ce5",
      "metadata": {
        "id": "de3c8ce5"
      },
      "source": [
        "##### Split   data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78b0a5d",
      "metadata": {
        "id": "b78b0a5d"
      },
      "outputs": [],
      "source": [
        "x = data.drop(columns='Class')\n",
        "y = data.Class\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8825a782",
      "metadata": {
        "id": "8825a782"
      },
      "source": [
        "#####  Scale data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e24959",
      "metadata": {
        "id": "d6e24959"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "x_train = pd.DataFrame(scaler.fit_transform(x_train))\n",
        "x_valid = pd.DataFrame(scaler.transform(x_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333ba55e",
      "metadata": {
        "id": "333ba55e"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de9ac4a",
      "metadata": {
        "id": "9de9ac4a"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c56959c7",
      "metadata": {
        "id": "c56959c7"
      },
      "outputs": [],
      "source": [
        "model  = keras.Sequential([\n",
        "    layers.Dense(4, 'relu', [33]),\n",
        "    layers.Dense(4, 'relu'),\n",
        "    # the final layer we use sigmoid activation function\n",
        "    layers.Dense(1, 'sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043f1d5f",
      "metadata": {
        "id": "043f1d5f"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd95c292",
      "metadata": {
        "id": "fd95c292"
      },
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2504a8d1",
      "metadata": {
        "id": "2504a8d1"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=1000,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2c5f4d",
      "metadata": {
        "id": "1f2c5f4d"
      },
      "outputs": [],
      "source": [
        "model_history = pd.DataFrame(history.history)\n",
        "model_history.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edf4f09",
      "metadata": {
        "id": "3edf4f09"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1,2, figsize=(13, 5))\n",
        "model_history[['loss', 'val_loss']].plot(ax=ax[0])\n",
        "model_history[['accuracy', 'val_accuracy']].plot(ax=ax[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696054f3",
      "metadata": {
        "id": "696054f3"
      },
      "source": [
        "# NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "k524UMDONLg_"
      },
      "id": "k524UMDONLg_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Bag of Words (BOW)"
      ],
      "metadata": {
        "id": "Lwc4TnzXNQ-V"
      },
      "id": "Lwc4TnzXNQ-V"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "uooaYELE7qCc"
      },
      "id": "uooaYELE7qCc",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2bd13659",
      "metadata": {
        "id": "2bd13659"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer1 = CountVectorizer()\n",
        "\n",
        "corpus = [\n",
        "          \"This is the first document.\",\n",
        "          \"And this is the second one.\"\n",
        "]\n",
        "X1 = vectorizer1.fit_transform(corpus)\n",
        "X1.toarray()"
      ],
      "metadata": {
        "id": "POsWXW-Kse57",
        "outputId": "8d790ecf-7e7b-4e59-b613-1c58390d97bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "POsWXW-Kse57",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 0, 0, 1, 1],\n",
              "       [1, 0, 0, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer1.vocabulary_"
      ],
      "metadata": {
        "id": "lr0YAs_otJOh",
        "outputId": "0ffdb821-dddd-449a-b621-e61178a436d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lr0YAs_otJOh",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 1,\n",
              " 'first': 2,\n",
              " 'is': 3,\n",
              " 'one': 4,\n",
              " 'second': 5,\n",
              " 'the': 6,\n",
              " 'this': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QRGdsLDlvsuO"
      },
      "id": "QRGdsLDlvsuO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF(Term Frequency-Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "9ijMN0VetVTS"
      },
      "id": "9ijMN0VetVTS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ TF-IDF(t, d) = TF(t, d) * IDF(t) $\n",
        "\n",
        "$ IDF(t) = log \\frac{1+N}{1+DF(t)} + 1 $\n",
        "\n",
        "\n",
        "*   TF(t, d) : the frequency of word t in document d;\n",
        "*   DF(t) : the number of documents containing word t.\n",
        "\n"
      ],
      "metadata": {
        "id": "5hIAtHX5vUTy"
      },
      "id": "5hIAtHX5vUTy"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "IRdBxuWXtNEk"
      },
      "id": "IRdBxuWXtNEk",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer2 = TfidfVectorizer()\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "X2.toarray()"
      ],
      "metadata": {
        "id": "IHFIh-1vxO_a",
        "outputId": "7b2223dc-0d04-4a70-f5f5-b9a2133c30a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IHFIh-1vxO_a",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.53309782, 0.53309782, 0.37930349, 0.        ,\n",
              "        0.        , 0.37930349, 0.37930349],\n",
              "       [0.47042643, 0.        , 0.        , 0.33471228, 0.47042643,\n",
              "        0.47042643, 0.33471228, 0.33471228]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding"
      ],
      "metadata": {
        "id": "N8rsq6Yvz4Fa"
      },
      "id": "N8rsq6Yvz4Fa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/CT608/Deep_Learning/blob/main/Pictures/93adea57dd44868dd97ce7378be4dd0.jpg?raw=true\n",
        "\" width=70%>"
      ],
      "metadata": {
        "id": "rx3sNyJV4pzb"
      },
      "id": "rx3sNyJV4pzb"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(11, 4, padding_idx=10)\n",
        "embedding.weight #this is similar to the right matrix of the above picture"
      ],
      "metadata": {
        "id": "1VbwWLVexuqN",
        "outputId": "1ed71dd1-be29-438e-c7d1-22d33da385b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1VbwWLVexuqN",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.7666, -1.1007, -1.0370, -0.8972],\n",
              "        [ 1.6282, -0.1722, -0.1903, -0.0537],\n",
              "        [-1.4381, -0.3847, -0.7910, -0.0032],\n",
              "        [ 2.2032, -0.1362, -0.4988, -0.8137],\n",
              "        [-0.8977, -1.3183,  0.2952, -1.8086],\n",
              "        [-0.3296, -0.8729,  0.6099,  0.1308],\n",
              "        [-1.4520, -0.8888,  0.8420, -0.4980],\n",
              "        [ 0.2580,  0.2150,  0.6035,  0.6584],\n",
              "        [-1.4233,  1.8499,  1.0712, -0.5191],\n",
              "        [-0.6894,  0.2017, -0.1428,  0.9833],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.LongTensor([7, 5, 2, 4, 10, 10, 10])\n",
        "embedding(input) #this is similar to the middle matrix of the above picture"
      ],
      "metadata": {
        "id": "V7mq-z9B8EFV",
        "outputId": "c6374cb4-6656-49e5-a45b-4e088a1a0935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "V7mq-z9B8EFV",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2580,  0.2150,  0.6035,  0.6584],\n",
              "        [-0.3296, -0.8729,  0.6099,  0.1308],\n",
              "        [-1.4381, -0.3847, -0.7910, -0.0032],\n",
              "        [-0.8977, -1.3183,  0.2952, -1.8086],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JxsQdz3NNIxB"
      },
      "id": "JxsQdz3NNIxB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN (Recurrent Neural Network)"
      ],
      "metadata": {
        "id": "d9XXUohoNTxL"
      },
      "id": "d9XXUohoNTxL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Recurrent Neural Network\n",
        "\n",
        "<img src=\"https://github.com/CT608/Deep_Learning/blob/main/Pictures/4f931c9e94382b6ce906f6694fc054b.png?raw=true\n",
        "\" width=50%>\n",
        "\n",
        "$ y_t = h_t = tanh(W_{ih}x_t + b_{ih} + W_{hh}h_{t-1} + b_{hh}) $\n",
        "\n",
        "\n",
        "\n",
        "*   Each hidden unit receives 2 inputs ($x_t,h_{t-1}$)\n",
        "*   $ tanh x = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$, it may cause vanishing or exploding gradient problem.\n",
        "\n",
        "<img src=\"https://github.com/CT608/Deep_Learning/blob/main/Pictures/tanhx.jpg?raw=true\" width=30%>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#code\n",
        "torch.nn.RNN()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k1matoyRNfI5"
      },
      "id": "k1matoyRNfI5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Long-Short Term Memory (LSTM)\n",
        "\n",
        "<img src=\"https://github.com/CT608/Deep_Learning/blob/main/Pictures/LSTM.jpg?raw=true\" width=70%>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   $c_{t-1}$: cell state at previous time step; \n",
        "*   $c_{t}$: cell state at current time step; \n",
        "*   $h_{t-1}$: activation from previous time step; \n",
        "*   $h_{t}$: activation for the next time step; \n",
        "*   $ f_t $: forget gate, controls which information is remembered and which is forgotten;\n",
        "*   $ i_t $: input gate, update the current cell state.\n",
        "\n",
        "\n",
        "```\n",
        "#code\n",
        "torch.nn.LSTM()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NZclwQ8VRJ_c"
      },
      "id": "NZclwQ8VRJ_c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gated Recurrent UNit (GRU)\n",
        "\n",
        "<img src=\"https://github.com/CT608/Deep_Learning/blob/main/Pictures/GRU.jpg?raw=true\" width=50%>\n",
        "\n",
        "A simple version of LSTM, performance between LSTM and GRU is close.\n",
        "\n",
        "\n",
        "```\n",
        "#code\n",
        "torch.nn.GRU()\n",
        "```"
      ],
      "metadata": {
        "id": "lGW2wfxjaOHX"
      },
      "id": "lGW2wfxjaOHX"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b5-BqVEE8tcD"
      },
      "id": "b5-BqVEE8tcD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "273.188px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Intro to DL.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}